2. Our problem statement is payload delivery with multi Unmanned Aerial Vehicles

3. So the UAV or quadrotor has to go to pickup locations to pickup parcels and drop it to the destination location.

4. Multi-UAVs provide various commercial applications such as 

5 So we will model our problem as mutiagent systems where each UAV will be modelled as one agent. A multi-agent system may contain combined human-agent teams.

6. So we solved our problem using reinforcment learning in multi agent setting. So under multiagent setting we are considering cooperative scenario where UAVS are cooperating to achieve a common goal.

7. Now reinforcement learning involves training an agent which interacts with its environment. The agent transitions between different scenarios of the environment referred to as states by performing actions. 

8. So we can consider quadrotors as agents and this open field as an environment.

9. So in Reinforcment learning setting actions taken by agents in return yield reward which could be positive or negative. The agents sole purpose is to maximize the total reward it collects over an episode. The actions taken in states are referred as policy of the agents which can be either represented by mu or pi.

10. So the Q-value is the value of a state with respect to action under policy π. In case of continuous environment like open field we approximate Q-values with a Neural Network.

11. So there are two ways of training agents, value based and policy based. Value based algorithms learns by trying to find each state's action value. Policy gradients learns in a more robust way by not trying to evaluate the value of each action - but by simply evaluating which action should it prefer. So the neural weights directly impacts the policy pi.

12. So the update rule is this . Here J is the performance measure. In Gradient Policy we want to increase performance, and therefore we wish to maximize the derivative of J, and not minimize it.

13. So now we will discuss Actor-Critic which works on policy gradients and it is made up of two sub-agents learning together : one learns the policy which should be acted by ( known as Actor), and the other learns the Q-Value of each state and action (known as Critic). So the neural network weight update equation is this.

14. On-Policy learning algorithms are the algorithms that evaluate and improve the same policy which is being used to select actions. Off-Policy learning algorithms evaluate and improve a policy that is different from Policy that is used for action selection. One advantage off policy methods provide are continuous exploration of the environment.

15. So the off-policy methods require experience reply buffer. Replay memory data set is what we’ll randomly sample from to train the network.

16. So Deep Deterministic Policy Gradient (DDPG) is an actor-critic technique consisting of two models: Actor and Critic. The actor is a policy network that takes the state as input and outputs the exact action, instead of a probability distribution over actions. The critic is a Q-value network that takes in state and action as input and outputs the Q-value. DDPG is an “off”-policy method and is used in a continuous action setting and is an improvement over the vanilla actor-critic. 

17. Multi-agent Deep Deterministic Policy Gradient (MADDPG) extends DDPG to an environment where multiple agents are coordinating to complete tasks with information. The approach MADDPG follows is of centralized training and decentralized execution. The critic in MADDPG learns a centralized action-value function. Meanwhile, multiple actors, one for each agent, are exploring and upgrading the policy parameters θi on their own.

18. So this is the pictorial representation of MADDPG's working.

19. So here are the equations of critic updates and actor updates.

20. Now we will shift towards safe RL. The aim of Safe Reinforcement learning is to create a learning algorithm that is safe while testing as well as during training. One approach is based on learning the policy in presence of hard constraints i.e the constraints should never be violated. In this work, we can directly add to the policy a safety layer that analytically solves an action correction formulation per each state.

21. A major challenge in using reinforcement learning is safety - control policies learned using reinforcement learning typically do not provide any safety guarantees. So we add a safety layer to the original policy which will execute action correction.

22. The analytical solution to this equation is found out and this “safe action” is used in training. For example in case of policy gradient methods, instead on the action a, the safe action a* is supplied to the update network call.

23.

24. So we also implemented MADDPG with PER. Data generated by Actor-Critics are stored in Replay Buffer and MADDPG uses uniform sampling. With PER, prioritized sampling is done so that important samples are more frequently drawn.

25. So now we will discuss experimental results. Currently we will discuss open world environment which is 2 dimensional.

26. So here is the demo of our environment with 3 UAVs. The UAVs are going to pick up the packages and deliver them to their destination location.

27. We conducted our experiments with number of UAVs set to 3,5 and 7. Also we compared our approach(Safe-MADDPG) with MADDPG and PER-MADDPG. So here we plotted mean rewards over episodes, when number of UAVs were set to 5. The safe one and the PER is having higher rewards than the original one.

28. Next we plotted collisions per 1000 episodes for all the 3 algorithms. So, as mentioned in figure, the collisions in Safe-MADDPG are getting reduced over training time. Here also number of UAVs are set to 5. Similar plot curves are obtained with 3 and 7 agents. Safe-MADDPG is efficiently learning the error correction over the episodes. However, for the other two, the graphs are near equal range.

29. Next we compared the collisions per 1000 episodes w.r.t. number of agents in the environment. The scaling of the environment is set to -1 to 1. An increment is observed with respect to N. The main reason is the scaling which is set fixed for all the simulations.

30. Next we compared the test-time collisions per 1000 episodes averaged over 5 runs w.r.t. number of agents in the environment. Here also, similar increment is observed with respect to N. The main reason is the scaling which is set fixed for all the simulations. However, safe-MADDPG is performing better during test-time among all three tested algorithms.

31. Now we will discuss the safe behaviour exhibit by the agents. We plotted the trajectories taken by the agents for N = 3 and 5. 2 plots are plotted for each N, one where episode gets least reward and the other where the agent episode gets maximum reward. Considering the case when N = 3, left figure depicts highest reward of 56.19. The green agent slightly adjusts the path while crossing near the orange agent.

32. Similar behaviour is achieved when N = 5. The lowest rewards achieved when N = 3,5 is when locations, whether pickup or drop, are nearby, as depicted in right figure.

33. So this concludes our work till now. The further work convert the environment from 2D system to 3D system. The 3D system will simulate real world factors. Further testing with other algorithms such as PPO(Proximal Policy Optimization).